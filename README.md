INTRODUCTION
============

Image synthesis is widely applied by GANs for a few years, the result of
GANs on medical images are relatively satisfied but lack of flexibility
and the precision in image details \[1\]. The raising of Diffusion
models are showing the supreme outperformance than GANs in Text-to-image
generation tasks, like DALL-E \[2\]. The images that generated by
Diffusion models are increasingly improving and have a trending that is
even better than human being's creations.\[3\] However, the original
Diffusion Models are suffering from slow reverse inference time cost and
the lack of stability of the result in each Markov Chain. This is not
ideal in medical domain image generation. Hence, here we applied a new
Denoising Diffusion models that can be adjusted and lower the noises in
every few steps in the reverse inference process. \[4\] are showing
delightful results by increasing step size T with this idea. And hereby,
we wish to use admitting a progressive lossy decompression scheme that
can be interpreted as a generalization of autoregressive decoding \[5\]
to further improve the performance of \[4\]'s model.

PROBLEM STATEMENT
=================

Rare diseases' medical scans are very hard to obtain in world-wide
range. Most of time, those images are belong to patients themselves or
some of the best hospitals, who would like not to share. Hence, by using
image synthesis from limited amount of rare medical scans that are
already publicly published, we can create more scans that are related to
certain diseases, which are un-restricted by the confidential agreement
between two parties. And using those model generated images, we can help
doctors or medical students to learn those diseases without too many
difficulties. The current model of GANs needs more computation resources
to train and the inference time is large than the Diffusion models
\[6\]. Here, we would like to try the state-of-art technique for image
synthesis by Denosing Diffusion model to solve this task.


PROPOSED TECHNIQUE
==================

Summary
-------

![Regular Diffusion models inference process from
\[4\]](images/c5c9dcb0f0b3faaedf68d63fdef437a4a64908f06991aca721ec6fc9bcfd6d11){width="8cm"}

Adding noises in each step of Markov Chain process until the data
distribution converges to a given prior, i.e., standard Gaussian
distribution \[10\].

From Figure 2, we are showing the process of how originally diffusion
models work in image synthesis problems. We use our target image and add
random Gaussian noise in each step to make our MRI scan into a purely
noisy image that has the same distribution with a Gaussian noise
distribution.

![Demonstration of adding Gaussian Noises into our MRI
image](images/de92016894f1ed3254742f70cb9535d52ff6cb02fc95f6ad0aea13e9318ada69){width="8cm"}

Forward pass procedure on one Brain MRI scan image.

However, the original method are suffering from arbitrary results from a
given Gaussian distribution prior by the reverse path. So \[5\] proposed
a novel idea to use UNet model that combines with ResNet, attention,
positional embeddings, and residual connection to downsampling the input
images into a much smaller dimension matrices. Then the model upsamples
the matrices into a segmentation feature map that captures the most
import features of our input, as the autoregressive decoding method
\[5\]. In our model, we will use this technique with huber loss function
to regenerate those four types of images in our MRI dataset.

Detailed Model Structure
------------------------

![Flow Chart of the
model](images/118c98de03fa5377aafe6e413d06bcff0140f8a272eb4abc35249ccbf59c9093.png){width="6cm"}

Flow chart of how the denoising diffusion model works.

Figure 4 shows the whole structure of our model. First we define the
same function of position embeddings like the one in the Transformers
paper \[17\]. The position of each step in the forward pass and backward
pass is important for the model to know when and where to stop. This
makes the neural network \"understand\" at which particular time step
(noise level) it is operating, for every image in a batch. And in the
next step, we use CNN for down sampling the input images from pure RGB
values to the segmentation maps. But for recognizing the images values,
we use two fundamental CNN block as ResNet \[18\] and ConvNext block
\[19\]. And By the popular and sucess of Transformers \[17\] bring us,
we also add attention module in our model. For measuring and retrieving
the most important features from all images feature maps. And finally,
we define our conditional U-Net model structure. The demonstration of
UNet is in Appendix Figure 13.

1.  Add one convolutional layer on the batch of images that are added
    noise and positional embeddings on each step during the markov
    chain.

2.  And we use 2 ResNet/ConvNext blocks with group norm + attention
    layer + residual connection + downsample operation for downsampling
    the original image RGB inputs.

3.  Then we use ResNet or ConvNext blocks with attention again for
    further downsampling.

4.  After we get into the lowest dimension of our downsampling stage, we
    appling upsampling by the same combination of blocks, such that CNN
    blocks + group norm + attention and residual connection. All those
    steps are for upsampling the features we got before.

5.  At the end, we apply one ResNet + convolutional layer for getting
    the output segmentation map.

After we got the segmentation feature map, we put the ouput values of
UNet into the Markov Chain process of forward pass and backward pass.
Then we use huber loss function to get the loss value from Markov Chain
equation for updating the parameters of our UNet model. This is the
process of training model. Now we introdcue the sampling part.

Inference and Sampling
----------------------

Here, we introduce the timesteps. From the original paper, it is the
number of denosing or add nosing steps of one simple forward or backward
diffusion procedure. The more timesteps it has in one step, the more
clearly the image could be. In the original paper, they use 2000
timesteps for one pass. Here, in our project, due to the limiation of my
GPU resources. We set the timesteps as 300. Then We start from time T =
300, and we sample pure Gaussian noise values, then we use our UNet
model to gradually denoising it, until the timesteps equals to 1 at the
beginning. algorithm shows in Figure 5.

![Sampling algorithms from
\[5\]](cc9b0c7ee57984183a0a543eafc36f88cd2e1613d3716ed81cc769d0c2610cce){width="8cm"}

We use for loop and the p-sample algorithm to denoising the Gaussian
random noise distribution at T = 300 to our target MRI scan image at T =
0.

In the best case, we will get a new and original MRI scan from the
random Gaussian noise as the model generate a brain scan for us.

EVALUATION METRICS
==================

In this project, we used Huber loss as the loss function and evaluation
metrics. The Huber loss function describes the penalty incurred by an
estimation procedure f. Huber (1964) defines the loss function piecewise
by
$$L_\delta(a)= \begin{cases}\frac{1}{2} a^2 & \text { for }|a| \leq \delta \\ \delta \cdot\left(|a|-\frac{1}{2} \delta\right), & \text { otherwise }\end{cases}$$
There is a reason why we didn't use SSIM for my evaluation metrics. Due
to the very limited GPU power on training our model. The image was
limited by a very small resolution as we discussed in the next section.
That made our model only can capture the overall shape. If we can use
more power GPU with more GPU memory, we can get more satisfied result.
Hence, only at that point, the SSIM evaluation metrics is meaningful.

PERFORMANCE EVALUATION AND RESULTS
==================================

Training Result
---------------

![image](de5ee2c853ece0cd53743cfe65fbb6c1feae95f93b010bfbc29a7cdcc20783e6){width="\\textwidth"}

Different hyperparameter makes big difference on the Huber loss.

In this project, we tried dozens of different hyperparameter
combinations until finally we successfully got the model started to
train. We used one Nvidia Tesla T4 with 16G GPU memory and continued to
train different model settings for more than 80 hours before we called
it the end.

For the best model by Figure 6, we set the image size as 44 by 44
pixels, channels = 1 as only black and white image, and batch size for
parameter updating as 80. We used Adam optimizer with learning rate 1e-3
and maximum 8000 epochs during the whole process. The minium huber loss
we got from the best model is $0.0126018$. And if any loss values less
than 0.015, the model has no meaningful result. The shape of generated
image is nothing like the target image.

![Validation Loss during the training of image size 42, batch size
80](3db12fe168830dfd5f015a289c9529380a65656f5b7961c0e86e6cb6c191bb9c){width="8cm"}

The model improving very fast at the beginning, and turn to slow after a
few hundreds epochs. But it still keep improving very slowly.

@ l \*10C c @ Model setting & count & mean & std & min & 25% & 50% & 75%
& max\
Image size 44, batch size 80 & 8000 & 0.0312 & 0.0381 & 0.0126 & 0.0224
& 0.0262 & 0.0306 &0.4902\
Image size 72, batch size 40 & 2600 &23.5068 &272.1552 &0.0164 &0.0266
&0.0327 &0.068 &5959.6943\

And for all the models we tried by Figure 6, the huber loss were so
different on different hyperparameter. We found that if we increase the
image size, then we must also increase the batch size. The ratio of
(Image size / batch size) must be sustained. If the ratio is decreasing
in the model design, the model won't improve after no matter how many
epochs we gave to it. This took me more than 4 hours training during the
night on a higher resolution target image hyperparameter setting.

![Validation Loss during the training of image size 72, batch size
40](78faef664297b251c52597c6a6537ddd960404c26426fc9fea22aef0b4b979e9){width="8cm"}

The model improving very fast at the beginning, and turn to slow after a
few hundreds epochs. But it still keep improving very slowly.

For one certain training with image size = 72 and batch size = 40, which
is the best hyperparameter we can get from the Google Colab Free
resources. In this model by Figure 9, we trained more than 2200 epochs,
then we got a surge of Loss at 2450 epoch up to nearly 6000 loss value.
However, during the first 2000 epochs, the loss values are around 0.025
by Figure 9.

![Validation Loss of y axis from 0 to 0.2 of image size 72, batch size
40](e0881d83358762a0fb51223055d44ddbeab7631a3a472c581ea3ac1aa699d662){width="8cm"}

We can see that the validation loss is decreasing but suddenly had a
surge around 2040 epochs.

Hence, for the training result by comparing those two model settings by
Table 1, we finalized our hyperparameter on Nvidia Tesla T4 with image
size 42, batch size 80, channels 1, and Adam optimizer with learning
rate 1e-3. Base on this setting, now we will show the inference results.

Inference Result
----------------

In this section, we take 3 different sampling results with the model
generated images form epoch 0, 120, and 585. The rest of epochs have
similar human evaluation performance, the only difference is the Huber
loss was still decreasing.

For the initial model without any training epochs, at epoch 0, we have
the inference sampling result at Figure 10. We can see that there is
only random Gaussian noises as it suppose to be. For Figure 11, at epoch
120, the shape of human head was becoming more clearly. We can see that
the contour of human skull and some shape of human mouth or jaw. For
Figure 12, at epoch 585, we can clearly see the four different types of
human Brain tumor or healthy brain MRI scans. The human eyes, skulls or
even the Amygdala is clear to observe. I didn't end up here, but the
rest of the epochs are lower the Huber loss, but there is no big
difference than the result at 585 epochs.

![Inference result on Epoch
0](59e2f7fc69975a3aec204de851ac9539a63dd6387cf1450fa031d8cdeca38d6f){width="5cm"}

We can see that there is only exist pure Gaussian random noises.

![Inference result on Epoch
120](b7bf6951d0623992c03de36b86db7093a019aec038182830b287d29bc92deeee.png){width="5cm"}

Model start to learn the shape of the input images.

![Inference result on Epoch
585](5225d6cafd9b03bc8de5597779f4640437dd3de797bf702978fed95d3146253f.png){width="5cm"}

The model already captured the essence of Human Brain MRI Scan.

CONCLUSION AND FUTURE SCOPE
===========================

From this project, we can see that the potential of generating rare
diseases with small number of images are valid. The Diffusion model gave
us a satisfied result even on a small resolution image input. And by
testing and trying all the combinations of hyperparameter, we can find
one setting that can be trained and improved by thousands of epochs. In
the meantime, improving the image resolution need more computation
resources is one obstacle that I currently have, but not for the whole
world or further investigation with funding. And we also noticed that
the certain ratio of (image size / batch size) is a critical training
signal that will indicate whether the model will successfully train or
not.

Therefore, in the future if we still have chance to continue this
project, I would like to use more powerful GPU with different
hyperparameter settings for getting a better result. And in that case,
we can apply SSIM evaluation metrics with the inference images with the
original input images to show the performance of our denoising diffusion
model.

99

H. Huang, P. S. Yu, και C. Wang, 'An Introduction to Image Synthesis
with Generative Adversarial Nets'. arXiv, 2018.

A. Ramesh κ.ά., 'Zero-Shot Text-to-Image Generation'. arXiv, 2021.

J. Yu κ.ά., 'Vector-quantized Image Modeling with Improved VQGAN'.
arXiv, 2021.

M. Özbey κ.ά., 'Unsupervised Medical Image Translation with Adversarial
Diffusion Models'. arXiv, 2022.

J. Ho, A. Jain, and P. Abbeel, 'Denoising Diffusion Probabilistic
Models'. arXiv, 2020.

S. U. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem, and T. C¸ ukur,
"Image synthesis in multi-contrast MRI with conditional generative
adversarial networks," IEEE Trans. Med. Imag., vol. 38, no. 10, pp.
2375--2388, 2019.

B. Yu, L. Zhou, L. Wang, Y. Shi, J. Fripp, and P. Bourgeat, "Ea-GANs:
Edge-aware generative adversarial networks for cross-modality MR image
synthesis," IEEE Trans. Med. Imag., vol. 38, no. 7, pp. 1750--1762,
2019.

A. Sharma and G. Hamarneh, "Missing MRI pulse sequence synthesis using
multi-modal generative adversarial network," IEEE Trans. Med. Imag.,
vol. 39, pp. 1170--1183, 2020.

G. Wang et al., "Synthesize high-quality multi-contrast magnetic
resonance imaging from multi-echo acquisition using multi-task deep
generative model," IEEE Trans. Med. Imag., vol. 39, no. 10, pp.
3089--3099, 2020.

L. Yang κ.ά., 'Diffusion Models: A Comprehensive Survey of Methods and
Applications'. arXiv, 2022.

F. A. Fardo, V. H. Conforto, F. C. de Oliveira, και P. S. Rodrigues, 'A
Formal Evaluation of PSNR as Quality Measurement Parameter for Image
Segmentation Algorithms'. arXiv, 2016.

J. Nilsson και T. Akenine-Möller, 'Understanding SSIM'. arXiv, 2020.

M. Mirza και S. Osindero, 'Conditional Generative Adversarial Nets'.
arXiv, 2014.

M.-Y. Liu, T. Breuel, και J. Kautz, 'Unsupervised Image-to-Image
Translation Networks'. arXiv, 2017.

J. Ho, A. Jain, και P. Abbeel, 'Denoising Diffusion Probabilistic
Models'. arXiv, 2020.

A. Jog, A. Carass, S. Roy, D. L. Pham, and J. L. Prince, "Random forest
regression for magnetic resonance image synthesis," Med. Image Anal.,
vol. 35, pp. 475--488, 2017.

J.A. Vaswani et al., 'Attention Is All You Need'. arXiv, 2017.

K. He, X. Zhang, S. Ren, and J. Sun, 'Deep Residual Learning for Image
Recognition'. arXiv, 2015.

Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, 'A
ConvNet for the 2020s'. arXiv, 2022.
